---
title: 给Python爬虫小白的入门级爬虫概述
date: 2016-10-12 20:03:37
tags: [Python, Crawler]
categories: Python
---

## 静态网页与动态网页
静态网页是指用户访问网页时，网站服务器直接把所存储的页面传递给用户。与之相对的，动态网页的内容往往是由网络应用动态产生的。在最初接触网络爬虫时，经常会出现爬虫爬取网页后，分析时却很蛋疼，你觉的非常正确的查找Beautiful Soup却一直返回空，这时候就要注意看看爬取的页面是否是动态生成的，直接上python自带的urllib或urllib2包括Requests往往无法获取动态生成的页面。
<br />

## 爬取工具
对于静态网页的爬取，可以选取的工具套装有Requests和Beautiful Soup，前者相对于Python自带的urllib, urllib2要易用的多。作者也称之为HTTP for Human。Requests直接托管在GitHub上，你可以从GitHub获取源码，其用法参看其官方文档。如果说Requests的主要任务是帮你的爬虫从网上搜刮下来网页的话，Beautiful Soup的作用就是帮你从网页中提取出有用的数据。对于爬取出来的数据，你可以将其保存到SQL数据库中，一个还比较好用的Python库是pymysql，可以利用它来实现你的爬虫脚本与MySQL数据库的交互。
对于动态网页的爬取，一个比较不折腾的方法是利用Selenium自动化工具。利用其提供的WebDriver，你可以直接开一个浏览器驱动程序来模拟爬取的过程，由于是直接用浏览器驱动程序来实现对Url的访问，动态产生的内容也就能够正常地加载出来的。当然，它有一些慢，而且有时候可能会出现一些奇怪的问题，但是对于爬虫初学者，利用Selenium是一个学习线路比较平缓，比较容易上手的选择。一个需要注意的是，Selenium WebDriver需要一些xpath的知识，当然，这是比较易学的。WebDriver也提供一些用来提取数据的方法，当然你也可以结合Beautiful Soup一起来对数据进行处理。同样的，对处理后的数据存入数据库是一个好选择。当然，在学习爬虫的早期，简单地将数据先存为json或者csv等文件而专注于爬取也是可以的。
<br />
## 爬取的流程
![](/images/20161012_1.jpg)
<br />
## 应对网站的反爬机制
爬虫往往因为会对网页造成大量的访问而使网站所有人比较头痛，因此网站开发人员会采用一些方法来识别请求是由真正的人类，还是盗版的爬虫机器人发出的。一般来说，爬虫机器人对网站的访问速度会大大的超过人类在浏览器上点鼠标的动作，因此，如果你的爬虫爬取速度过快，很容易被网站禁止。解决方法就是导入Python的time库，通过time.sleep()来指定一个爬取频率。另外在真正浏览器访问服务器的时候，所发送的请求数据有一个头部分，其中User Agent用来指明访问者信息。在用Requests爬虫发送请求时，这个头就显示的是requests，这就比较尴尬了。网站不把爬虫拍下来都不好意思，所以伪造User Agent是爬网站时的常用伎俩。当然，如果一个User Agent对网站访问过于频繁，网站也可能会认为这个请求来源于一个爬虫，因此建立一个User Agent池，随机从中取User Agent来使用也很常用。最后，我们的ip地址也有可能会暴露我们的程序是爬虫，所以用一个ip来做爬虫往往也会被网站所屏蔽，此时可以使用代理ip来帮助我们隐藏身份，网上会有很多免费的代理ip，当然你也可以去买，买的代理ip比免费的要稳定的多。从网上获得代理ip后(这个行为可以用爬虫爬，也有一些网站提供接口可以直接使用。)，我们可以先利用ping命令验证这些ip的可用性，然后把他们存入ip池中，随即从中取出使用。由于免费的ip往往容易失效，所以动态地更新ip池是有必要的。

虽然利用以上的方法可以很好地掩护我们的爬虫，可是有些时候网站就是会出现一些验证码来阻碍我们的爬取。这个时候我们就...放弃吧。开玩笑，验证码是网站对抗爬虫的最后也是最强的一道屏障，很多网站的验证码已经变态到人类也很难识别了，比如某卖火车票网站。面对验证码，我们能做些什么呢？首先我们可以利用Python用于图像处理的库opencv，opencv自带了验证码识别，识别能力大概是：
![](/images/20161012_2.jpg)
这个识别能力非常的捉鸡，一般的网站也基本没有用这种验证码的。要处理比较不容易识别的验证码，我们就只能使用一些图像处理的算法了，简单的处理流程如下：
![](/images/20161012_3.jpg)
一般来说，经过这些处理过程，验证码的识别率会达到一个可接受的大小。但是我们如何进一步提高识别正确率呢，一些机器学习的算法这时候就开始起作用了，可以试试Pytesseract，GitHub主页有一个训练的教程，然而我最初做这个的时候莫名有一般的训练失败，祝你好运:-)
<br />
## 进阶与展望（这部分大多是我瞎逼逼。。。）
如果采用多线程的方法可以较好地提高爬取效率。后期可以尝试用这个方法加快爬取速度。进一步可以学习一些爬虫框架，例如scrapy。利用分布式爬虫可以做出大规模的爬取，通过Hadoop可以对大数据存储和处理。对于这些数据，Python有一些做数据分析的库，比如numpy, pandas, matplotlib等等。用这些数据我们可以建立一些统计、数学模型，可以用于人工智能领域。
